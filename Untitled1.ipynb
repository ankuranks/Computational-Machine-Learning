{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100000.jpg']\n",
      "<open file 'c:/Anu/100000.jpg', mode 'r' at 0x0000000009CDDED0>\n",
      "����4�Exif\u0000\u0000II*\u0000\b\u0000\u0000\u0000\u0015\u0000\u000f\u0001\u0002\u0000\n",
      "\n",
      "False\n",
      "['\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02\\x00']\n",
      "[ '\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02']\n",
      "[[ '\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02']]\n",
      "hmmm\n",
      "[[ '\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02']]\n",
      "i is\n",
      "0\n",
      "float again\n",
      "[[ '\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02']]\n",
      "data is\n",
      "[array([ '\\xff\\xd8\\xff\\xe14\\x8dExif\\x00\\x00II*\\x00\\x08\\x00\\x00\\x00\\x15\\x00\\x0f\\x01\\x02'], \n",
      "      dtype='|S26')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ÿØÿá4�Exif",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9dd175ba6df5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mphotos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;31m#content = list_data()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-9dd175ba6df5>\u001b[0m in \u001b[0;36mload_f\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m                                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data is'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;31m# [x for x in float1]#map(float,float1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ÿØÿá4�Exif"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy import array, zeros, mean, std, sort, add, subtract, divide, dot, sqrt, arange, random\n",
    "from numpy import linalg as la\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import numpy as np\n",
    "import urllib, urlparse\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def load_f():\n",
    "    mypath = 'c:/Anu/';\n",
    "    onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "    onlyfiles = [ f for f in listdir(mypath)]\n",
    "    print onlyfiles\n",
    "    data = np.zeros((1,128)) \n",
    "    data = np.matrix(data)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for f in os.listdir(mypath):\n",
    "        \n",
    "        j = j + 1\n",
    "        if j % 10  != 0:\n",
    "            if isfile(join(mypath,f)):\n",
    "                with open(join(mypath,f)) as f1:\n",
    "                    print(f1)\n",
    "                    for line in f1:\n",
    "                        \n",
    "                        print(line)\n",
    "                        print(line.isspace())\n",
    "                        if not line.isspace():\n",
    "                            #floats = map(float, line.split())\n",
    "                            print(line.split())\n",
    "                            floats = [x for x in line.split()]\n",
    "                            #print(floats[0])\n",
    "                            floats = np.asarray(floats)\n",
    "                            # print floats.shape\n",
    "                            print(floats)\n",
    "                            float1 = np.matrix(floats)\n",
    "                            print(float1)\n",
    "                            float1 = np.asarray(float1)\n",
    "                            print('hmmm')\n",
    "                            print(float1)\n",
    "                            print('i is')\n",
    "                            print(i)\n",
    "                            if i > 0:\n",
    "                                data = np.concatenate((data,float1),axis=0)\n",
    "                            else:\n",
    "                                #data[i,:] = map(float,float1)\n",
    "                                print('float again')\n",
    "                                print(float1)\n",
    "                                data2= [x for x in float1]\n",
    "                                print('data is')\n",
    "                                print(data2)\n",
    "                                data[i,:]=data2# [x for x in float1]#map(float,float1)\n",
    "                            i += 1\n",
    "   \n",
    "    return data\n",
    "\n",
    "def genvocabulary(orig_datapoints,n_cluster,max_itr):\n",
    "    clust_centers, labels = mykmeansplusplus(orig_datapoints,n_cluster,max_itr)\n",
    "    return clust_centers   \n",
    "\n",
    "def mykmeansplusplus (datapoints,n_cluster,max_iter):\n",
    "    data_x_shape = datapoints.shape[0]\n",
    "    random_index = np.random.randint(data_x_shape, size =1)\n",
    "    initial_clust_center = datapoints[random_index]\n",
    "    weight = np.zeros((datapoints.shape[0]))\n",
    "    index = np.zeros((datapoints.shape[0]))\n",
    "\n",
    "    for i in range(0,(n_cluster - 1)):\n",
    "        Y= cdist(datapoints, initial_clust_center, metric='euclidean', p=2, V=None, VI=None, w=None)\n",
    "        min_dist = np.amin(Y, axis=1) \n",
    "        min_center = np.argmin(Y, axis=1)\n",
    "        min_dist_sum = np.sum(min_dist, axis=0)\n",
    "        min_squared_dist_sum = np.sum(min_dist**2, axis=0)\n",
    "        min_dist = min_dist**2\n",
    "        #print('Inside Min')\n",
    "        #print(min_dist)\n",
    "        min_dist_prob = min_dist / min_squared_dist_sum\n",
    "        next_index = np.random.choice(datapoints.shape[0],1,p=min_dist_prob)\n",
    "        initial_clust_center = np.concatenate((initial_clust_center,datapoints[next_index]),axis=0)      \n",
    "    #print initial_clust_center\n",
    "    initial_class = np.zeros((datapoints.shape[0]))\n",
    "    for j in range(0,max_iter):\n",
    "        Z= cdist(datapoints, initial_clust_center, metric='euclidean', p=2, V=None, VI=None, w=None)\n",
    "        target_dist = np.amin(Z, axis=1)  \n",
    "        target_class = np.argmin(Z, axis=1)\n",
    "        for i in range(0,n_cluster):\n",
    "            values = datapoints[target_class == i]\n",
    "            initial_clust_center[i,:] = np.mean(values, axis = 0)\n",
    "        if np.array_equal(initial_class,target_class):\n",
    "            print j\n",
    "            print 'converged'\n",
    "            break\n",
    "        else:\n",
    "            initial_class = target_class\n",
    "\n",
    "    return initial_clust_center, target_class  \n",
    "        \n",
    "\n",
    "def list_data():\n",
    "    dirname='C:/Anu'\n",
    "    photos = {}\n",
    "    filelist = os.listdir(dirname)\n",
    "    for filename in filelist:\n",
    "\n",
    "        file_id = ''\n",
    "        m = re.search('(.*?)\\..*', filename)\n",
    "        if (m != None):\n",
    "            file_id = m.group(1)\n",
    "\n",
    "        #print(\"Reading \" + filename)\n",
    "        f = open(dirname + '/' + filename, 'r')\n",
    "        content = []\n",
    "        for line in f:\n",
    "            print('Jai sai')\n",
    "            #f=line.split()\n",
    "            print(f)\n",
    "            vec = [x for x in line.split()]\n",
    "            #vec = map(float, line.split())\n",
    "            content.append(vec)\n",
    "        content = array(content).flatten()\n",
    "        photos[file_id] = content\n",
    "        f.close() \n",
    "\n",
    "    return photos\n",
    "content = load_f()\n",
    "#content = list_data()\n",
    "print('hello')\n",
    "print(content)\n",
    "print('hello2')\n",
    "from sklearn import preprocessing\n",
    "#content = preprocessing.scale(content,axis = 0, with_mean=True, with_std=True)\n",
    "centroids = genvocabulary(content,100,10)\n",
    "#fvector = bof_data(centroids,10)\n",
    "#In [42]:\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "#fvector1 = bof_data(centroids,51)\n",
    "#neigh.fit(f_vector, fvector1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.63606852,  3.02192496,  5.48796085,  2.01163132],\n",
       "       [ 4.68855572,  3.2494003 ,  1.49011744,  0.17622439],\n",
       "       [ 6.06148316,  2.87006984,  4.46699877,  1.46126746]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "def getPermutation(totalRange,numberElements):\n",
    "    random_seed = 10312003\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    permutation = rng.permutation(totalRange)\n",
    "    return permutation[:numberElements]\n",
    "\n",
    "\n",
    "def onlineKmeans(X,k=3,b=30,maxiter=1000):\n",
    "    centroids = X[getPermutation(len(X),k)]\n",
    "    pointsPerClusters = np.zeros([k,1])\n",
    "    for i in range(maxiter):\n",
    "        M=X[getPermutation(len(X),b)]\n",
    "        distances = pairwise_distances(M, centroids, metric='euclidean')\n",
    "        nearestCenters = np.argmin(distances, axis=1)\n",
    "        for iter, x in enumerate(M):\n",
    "            centerIndex = nearestCenters[iter]\n",
    "            pointsPerClusters[centerIndex] = pointsPerClusters[centerIndex] + 1\n",
    "            eta = 1/pointsPerClusters[centerIndex]\n",
    "            centroids[centerIndex] = (1 - eta)*centroids[centerIndex] + eta * x\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X=iris.data\n",
    "random_seed = 10312003\n",
    "rng = np.random.RandomState(random_seed)\n",
    "permutation = rng.permutation(len(X))\n",
    "X = X[permutation]\n",
    "onlineKmeans(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-59e78e9a0d6a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-59e78e9a0d6a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    conda create -n py35 python=3.5\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda create -n py35 python=3.5\n",
    "source activate py35\n",
    "conda install notebook ipykernel\n",
    "ipython kernel install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def getPermutation(totalRange,numberElements):\n",
    "    random_seed = 10312003\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    permutation = rng.permutation(totalRange)\n",
    "    return permutation[:numberElements]\n",
    "\n",
    "def onlineKmeans(X,k=3,b=30,maxiter=1000):\n",
    "    centroids = X[getPermutation(len(X),k)]\n",
    "    pointsPerClusters = np.zeros([k,1])\n",
    "    for i in range(maxiter):\n",
    "        M=X[getPermutation(len(X),b)]\n",
    "        distances = pairwise_distances(M, centroids, metric='euclidean')\n",
    "        nearestCenters = np.argmin(distances, axis=1)\n",
    "        for iter, x in enumerate(M):\n",
    "            centerIndex = nearestCenters[iter]\n",
    "            pointsPerClusters[centerIndex] = pointsPerClusters[centerIndex] + 1\n",
    "            eta = 1/pointsPerClusters[centerIndex]\n",
    "            centroids[centerIndex] = (1 - eta)*centroids[centerIndex] + eta * x\n",
    "\n",
    "    return centroids\n",
    "\n",
    "from timeit import Timer\n",
    "from functools import partial\n",
    "from memory_profiler import memory_usage\n",
    "import timeit\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def profile_memory_and_time(function, *args, **kwargs):\n",
    "    #print \"args is \",args\n",
    "    start_time = timeit.default_timer()\n",
    "    memory, return_val = memory_usage((function, (args), kwargs), max_usage=True, retval=True)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    return memory[0], elapsed,return_val\n",
    "\n",
    "iris = load_iris()\n",
    "X=iris.data\n",
    "random_seed = 10312003\n",
    "rng = np.random.RandomState(random_seed)\n",
    "permutation = rng.permutation(len(X))\n",
    "X = X[permutation]\n",
    "ourMemory,ourTime,ourCentroids= profile_memory_and_time(onlineKmeans, X, k=3, b=33, maxiter=1000)\n",
    "#print ourMemory,ourTime,ourCentroids\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatch=MiniBatchKMeans(n_clusters=3,max_iter=100,batch_size=33)\n",
    "memory, time, rval = profile_memory_and_time(minibatch.fit,X)\n",
    "#print memory, time, rval.cluster_centers_\n",
    "\n",
    "inputSizesToGenerate = [[2**8, 32],[2**10, 32],[2**12, 32],[2**14, 32]]\n",
    "                        #[2**16, 32],[2**18, 32],[2**20, 32],[2**22, 32]];\n",
    "                        #[2**24, 32],[2**26, 32],[2**28, 32],[2**30, 32],[2**32, 32]];\n",
    "\n",
    "scaler = StandardScaler()\n",
    "plt.ion()\n",
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "\n",
    "for num_samples, num_dimension in inputSizesToGenerate:\n",
    "    print \"Running for {0} samples of dimension {1}\".format(num_samples, num_dimension)\n",
    "    X,y = make_blobs(n_samples=num_samples, n_features=num_dimension, centers=6)\n",
    "    #print \"Mean before scaling :\\n%s\"%X.mean(axis=0)\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    #print \"Mean after scaling :\\n%s\"%X_scaled.mean(axis=0)\n",
    "    #print \"Mean after scaling :\\n%s\"%X.mean(axis=0)\n",
    "    # run onlinekmeans and scikitkmeans\n",
    "    \n",
    "    ourMemory,ourTime,ourCentroids= profile_memory_and_time(onlineKmeans, X, k=3, b=33, maxiter=1000)\n",
    "    #print  ourMemory,ourTime,ourCentroids\n",
    "    memory, time, rval = profile_memory_and_time(minibatch.fit,X)\n",
    "    #print  memory, time, rval.cluster_centers_\n",
    "    #plt.scatter(num_samples, ourMemory,color=\"green\")\n",
    "    plt.scatter(num_samples, memory,color=\"red\")\n",
    "    plt.show()\n",
    "    plt.savefig('memory.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

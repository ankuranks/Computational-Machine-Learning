{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\documentclass[final,leqno,onefignum,onetabnum]{siamltex1213}\n",
    "\\usepackage[space]{grffile}\n",
    "\\usepackage{bmpsize}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{epsfig,subfig}\n",
    "\n",
    "\\graphicspath{C:/Users/Anubha/Desktop/}\n",
    "\n",
    "\\title{K-means , Bag of features} \n",
    "\\author{Ankur Chaudhary\\thanks\\email{ankur.chaudhary@nyu.edu}, {Courant Institute of Mathematical Sciences, NYU, NY}}\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\\begin{abstract}\n",
    "The objective of this assignment is to understand and implement k-means, k-means++, bag of features in our projects and Iris data set in order to enhance our understanding of the project.\n",
    "\\end{abstract}\t\n",
    "\t\n",
    "\\section{myKMeans, myKMeans++ and K-Means cluster}\n",
    "I implemented k-means with k=3 and tweaked with different values of k. I realized that if k large we have more clusters and every item in the cluster is closer to its cluster's centroid.The higher values of K means less is the distortion. However, the value should not be too high as well. As a rule of thumb the most optimum value of k in a K-means program is around square root of (0.5 * Number of Observations).Some of the plots of k-means are given below\n",
    "\n",
    "While implementing K-means++, I also observed that distortion reduced as K increased until it converged to a value. The value of this point is around square root of (0.5 * Number of Observations). I also compared values of K-means++ and K-means cluster. K-means cluster is faster due to several factors like numpy arrays which make it extremely fast as compared to updating the values using custom made loops. A good coding practice that I found in the cluster implementation was the use of package functions. The profiler tool did point places where the code was taking time to run. I applied code optimization techniques for my program. After spending time in development I realized that it would have been easier if I had a ready list of library functions in numpy. After applying the tricks used in standard code, I was able to improve both speed and quality of my code by reducing the output time and reducing lines of code. I experienced approximately 20 percent improvement in the performance.\n",
    "I also plotted the distortion curve. \n",
    "The following figures showcase K-Mean curves for k=3,5,7 and also the distortion curve.\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{First.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Second.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Third.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Fourth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Fifth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Sixth.png}\n",
    "\n",
    "\\section{Struggles with myKMeans,myKmeans++ and Kmeans}\n",
    "I had to struggle with K-means code in updating centroid co-ordinates. Understanding the concept of K-Means in detail helped in rectifying it.\n",
    "There was also struggle in visualizing K-Means++. So, I implemented a basic version based on pure python and kept on improving it by replacing custom codes with library functions.\n",
    " There were also issues in the code as some of the functions in python are also present in numpy (but with different inputs). So, I had to ensure that the right methods were getting picked.\n",
    "Passing the code in profiler tool also showed some of the longer consuming taking lines. I replaced those components with numpy functions/arrays.\n",
    "\n",
    "\\section{Vocabulary and Bag of Features }\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Seventh.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Eight.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Ninth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Tenth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Eleventh.png}\n",
    "\n",
    "\\section{Making Vocabulary and Bag of Features}\n",
    "I built Vocabulary and Bag of visual words using SIFT, myKMeans++ and other custom feature extractors. Bag of images was converted into Bag of features. I used tricks of \"feature extraction\" from sklearn to improve the code both in terms of quality and speed. The overall efficiency improved by 25 percent. I noticed that feature extraction contains functions like tf-idf which help in better performance over custom functions, which rely on manually incremented loops.\n",
    "\n",
    "I also tweaked with bag of words extraction using text data. After modifying the algorithm to KNN from Naïve-Bayes, I printed all the outputs and realized that all the outputs were same just that the accuracy in test section was more than Naïve-Bayes,however it was still less than SVM. The snapshots from the exercise are displayed above.\n",
    "This exercise was especially useful in understanding \"feature extraction\" coding techniques which helped in improving my custom functions later on. It was also a good exercise to learn parameter tuning using grid search and deciding on optimal values of parameters.\n",
    "\n",
    "\\section{Struggles with BOF and Vocabulary}\n",
    "Understanding the concept of building bag of visual words and creating vocabulary took a lot of time.\n",
    "There was also struggle in understanding feature extraction using SIFT.\n",
    "There were issues in versions of OpenCV (latest versions of OpenCV do not support SIFT, so I took version 2.4.9). There were issues in installation of other packages as well.\n",
    "\n",
    "\\section{First Approach to Project}\n",
    "After doing experiments and implementations in the previous sections, I played around with the code to find optimum settings. The details of some of these tweaks can be found on Test and Trial iPython notebook on GitHub.\n",
    "\n",
    "\\section{Feedback}\n",
    "\\begin{itemize}\n",
    "\\item I took 6 days to complete this assignment\n",
    "\\item I am still working with grid-search to find near optimal settings. It needs more practice.\n",
    "\\item No, however, I would like to point out that Stochastic gradient descent is something which I would like to study more.\n",
    "\\item If Professor can take up Stochastic gradient descent for some time in the class it'll be great.\n",
    "\\end{itemize}\n",
    "\n",
    "\\begin{thebibliography}{1}\n",
    "\t\\bibitem{SaPeVeMe} {\\sc D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding.}{\\em In Proceedings\n",
    "of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007,} New Orleans, Louisiana, USA, January 7-9, 2007, pages 1027–1035, 2007. 2, 2.3,\n",
    "\t\n",
    "\t\\bibitem{ArZi} {\\sc Relja Arandjelovi´c, Andrew Zisserman}, {\\em All about VLAD\n",
    "}, Department of Engineering Science, University of Oxford\n",
    "\t\n",
    "\\end{thebibliography}\n",
    "\\bigskip\n",
    "\\end{document}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\documentclass[final,leqno,onefignum,onetabnum]{siamltex1213}\n",
    "\\usepackage[space]{grffile}\n",
    "\\usepackage{bmpsize}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{epsfig,subfig}\n",
    "\n",
    "\\graphicspath{C:/Users/Anubha/Desktop/}\n",
    "\n",
    "\\title{K-means , Bag of features} \n",
    "\\author{Ankur Chaudhary\\thanks\\email{ankur.chaudhary@nyu.edu}, {Courant Institute of Mathematical Sciences, NYU, NY}}\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\\begin{abstract}\n",
    "The objective of this assignment is to understand and implement k-means, k-means++, bag of features in our projects and Iris data set in order to enhance our understanding of the project.\n",
    "\\end{abstract}\t\n",
    "\t\n",
    "\\section{myKMeans, myKMeans++ and K-Means cluster}\n",
    "I implemented k-means with k=3 and tweaked with different values of k. I realized that if k large we have more clusters and every item in the cluster is closer to its cluster's centroid.The higher values of K means less is the distortion. However, the value should not be too high as well. As a rule of thumb the most optimum value of k in a K-means program is around square root of (0.5 * Number of Observations).Some of the plots of k-means are given below\n",
    "\n",
    "While implementing K-means++, I also observed that distortion reduced as K increased until it converged to a value. The value of this point is around square root of (0.5 * Number of Observations). I also compared values of K-means++ and K-means cluster. K-means cluster is faster due to several factors like numpy arrays which make it extremely fast as compared to updating the values using custom made loops. A good coding practice that I found in the cluster implementation was the use of package functions. The profiler tool did point places where the code was taking time to run. I applied code optimization techniques for my program. After spending time in development I realized that it would have been easier if I had a ready list of library functions in numpy. After applying the tricks used in standard code, I was able to improve both speed and quality of my code by reducing the output time and reducing lines of code. I experienced approximately 20 percent improvement in the performance.\n",
    "I also plotted the distortion curve. \n",
    "The following figures showcase K-Mean curves for k=3,5,7 and also the distortion curve.\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{First.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Second.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Third.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Fourth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Fifth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Sixth.png}\n",
    "\n",
    "\\section{Struggles with myKMeans,myKmeans++ and Kmeans}\n",
    "I had to struggle with K-means code in updating centroid co-ordinates. Understanding the concept of K-Means in detail helped in rectifying it.\n",
    "There was also struggle in visualizing K-Means++. So, I implemented a basic version based on pure python and kept on improving it by replacing custom codes with library functions.\n",
    " There were also issues in the code as some of the functions in python are also present in numpy (but with different inputs). So, I had to ensure that the right methods were getting picked.\n",
    "Passing the code in profiler tool also showed some of the longer consuming taking lines. I replaced those components with numpy functions/arrays.\n",
    "\n",
    "\\section{Vocabulary and Bag of Features }\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Seventh.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Eight.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Ninth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Tenth.png}\n",
    "\n",
    "\\includegraphics[width=4.5cm,height=4cm,angle=0]{Eleventh.png}\n",
    "\n",
    "\\section{Making Vocabulary and Bag of Features}\n",
    "I built Vocabulary and Bag of visual words using SIFT, myKMeans++ and other custom feature extractors. Bag of images was converted into Bag of features. I used tricks of \"feature extraction\" from sklearn to improve the code both in terms of quality and speed. The overall efficiency improved by 25 percent. I noticed that feature extraction contains functions like tf-idf which help in better performance over custom functions, which rely on manually incremented loops.\n",
    "\n",
    "I also tweaked with bag of words extraction using text data. After modifying the algorithm to KNN from Naïve-Bayes, I printed all the outputs and realized that all the outputs were same just that the accuracy in test section was more than Naïve-Bayes,however it was still less than SVM. The snapshots from the exercise are displayed above.\n",
    "This exercise was especially useful in understanding \"feature extraction\" coding techniques which helped in improving my custom functions later on. It was also a good exercise to learn parameter tuning using grid search and deciding on optimal values of parameters.\n",
    "\n",
    "\\section{Struggles with BOF and Vocabulary}\n",
    "Understanding the concept of building bag of visual words and creating vocabulary took a lot of time.\n",
    "There was also struggle in understanding feature extraction using SIFT.\n",
    "There were issues in versions of OpenCV (latest versions of OpenCV do not support SIFT, so I took version 2.4.9). There were issues in installation of other packages as well.\n",
    "\n",
    "\\section{First Approach to Project}\n",
    "After doing experiments and implementations in the previous sections, I played around with the code to find optimum settings. The details of some of these tweaks can be found on Test and Trial iPython notebook on GitHub.\n",
    "\n",
    "\\section{Feedback}\n",
    "\\begin{itemize}\n",
    "\\item I took 6 days to complete this assignment\n",
    "\\item I am still working with grid-search to find near optimal settings. It needs more practice.\n",
    "\\item No, however, I would like to point out that Stochastic gradient descent is something which I would like to study more.\n",
    "\\item If Professor can take up Stochastic gradient descent for some time in the class it'll be great.\n",
    "\\end{itemize}\n",
    "\n",
    "\\begin{thebibliography}{1}\n",
    "\t\\bibitem{SaPeVeMe} {\\sc D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding.}{\\em In Proceedings\n",
    "of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007,} New Orleans, Louisiana, USA, January 7-9, 2007, pages 1027–1035, 2007. 2, 2.3,\n",
    "\t\n",
    "\t\\bibitem{ArZi} {\\sc Relja Arandjelovi´c, Andrew Zisserman}, {\\em All about VLAD\n",
    "}, Department of Engineering Science, University of Oxford\n",
    "\t\n",
    "\\end{thebibliography}\n",
    "\\bigskip\n",
    "\\end{document}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

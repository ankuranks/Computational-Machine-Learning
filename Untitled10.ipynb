{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def getPermutation(totalRange,numberElements):\n",
    "    random_seed = 10312003\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    permutation = rng.permutation(totalRange)\n",
    "    return permutation[:numberElements]\n",
    "\n",
    "def onlineKmeans(X,k=3,b=30,maxiter=1000):\n",
    "    centroids = X[getPermutation(len(X),k)]\n",
    "    pointsPerClusters = np.zeros([k,1])\n",
    "    for i in range(maxiter):\n",
    "        M=X[getPermutation(len(X),b)]\n",
    "        distances = pairwise_distances(M, centroids, metric='euclidean')\n",
    "        nearestCenters = np.argmin(distances, axis=1)\n",
    "        for iter, x in enumerate(M):\n",
    "            centerIndex = nearestCenters[iter]\n",
    "            pointsPerClusters[centerIndex] = pointsPerClusters[centerIndex] + 1\n",
    "            eta = 1/pointsPerClusters[centerIndex]\n",
    "            centroids[centerIndex] = (1 - eta)*centroids[centerIndex] + eta * x\n",
    "    return centroids\n",
    "\n",
    "print('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "from functools import partial\n",
    "from memory_profiler import memory_usage\n",
    "import timeit\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "print('hello')\n",
    "def profile_memory_and_time(function, *args, **kwargs):\n",
    "    #print \"args is \",args\n",
    "    start_time = timeit.default_timer()\n",
    "    memory, return_val = memory_usage((function, (args), kwargs), max_usage=True, retval=True)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print('m here')\n",
    "    return memory[0], elapsed,return_val\n",
    "print('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X=iris.data\n",
    "random_seed = 11112014\n",
    "rng = np.random.RandomState(random_seed)\n",
    "permutation = rng.permutation(len(X))\n",
    "X = X[permutation]\n",
    "ourMemory,ourTime,ourCentroids= profile_memory_and_time(onlineKmeans, X, k=3, b=33, maxiter=100)\n",
    "print ourMemory,ourTime,ourCentroids\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minibatch=MiniBatchKMeans(n_clusters=3,max_iter=100,batch_size=33)\n",
    "memory, time, rval = profile_memory_and_time(minibatch.fit,X)\n",
    "#print memory, time, rval.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSizesToGenerate = [[2**8, 32],[2**10, 32],[2**12, 32],[2**14, 32]]\n",
    "                        #[2**16, 32],[2**18, 32],[2**20, 32],[2**22, 32]];\n",
    "                        #[2**24, 32],[2**26, 32],[2**28, 32],[2**30, 32],[2**32, 32]];\n",
    "\n",
    "scaler = StandardScaler()\n",
    "plt.ion()\n",
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "\n",
    "for num_samples, num_dimension in inputSizesToGenerate:\n",
    "    print \"Running for {0} samples of dimension {1}\".format(num_samples, num_dimension)\n",
    "    X,y = make_blobs(n_samples=num_samples, n_features=num_dimension, centers=6)\n",
    "    #print \"Mean before scaling :\\n%s\"%X.mean(axis=0)\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    #print \"Mean after scaling :\\n%s\"%X_scaled.mean(axis=0)\n",
    "    #print \"Mean after scaling :\\n%s\"%X.mean(axis=0)\n",
    "    # run onlinekmeans and scikitkmeans\n",
    "    \n",
    "    ourMemory,ourTime,ourCentroids= profile_memory_and_time(onlineKmeans, X, k=3, b=33, maxiter=1000)\n",
    "    #print  ourMemory,ourTime,ourCentroids\n",
    "    memory, time, rval = profile_memory_and_time(minibatch.fit,X)\n",
    "    #print  memory, time, rval.cluster_centers_\n",
    "    #plt.scatter(num_samples, ourMemory,color=\"green\")\n",
    "    plt.scatter(num_samples, memory,color=\"red\")\n",
    "    plt.show()\n",
    "    plt.savefig('memory.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from timeit import Timer\n",
    "from functools import partial\n",
    "from memory_profiler import memory_usage\n",
    "import timeit\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def onlineKmeans(dataSet,k=3,b=30,maxiter=1000):\n",
    "    centroids = dataSet[Permutations(k,len(dataSet))]\n",
    "    pointsPerClusters = np.zeros([k,1])\n",
    "    for i in range(maxiter):\n",
    "        M=dataSet[Permutations(b,len(dataSet))]\n",
    "        distances = pairwise_distances(M, centroids, metric='euclidean')\n",
    "        nearestCenters = np.argmin(distances, axis=1)\n",
    "        for iter, x in enumerate(M):\n",
    "            centerIndex = nearestCenters[iter]\n",
    "            pointsPerClusters[centerIndex] = pointsPerClusters[centerIndex] + 1\n",
    "            eta = 1/pointsPerClusters[centerIndex]\n",
    "            centroids[centerIndex] = (1 - eta)*centroids[centerIndex] + eta * x\n",
    "    return centroids\n",
    "def Permutations(noElements,range):\n",
    "    random_seed = 10312003\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    permutation = rng.permutation(range)\n",
    "    return permutation[:noElements]\n",
    "\n",
    "def profiler(function, *arguments, **kwargs):\n",
    "    #print \"args is \",args\n",
    "    start_time = timeit.default_timer()\n",
    "    memory, return_val = memory_usage((function, (arguments), kwargs), max_usage=True, retval=True)\n",
    "    timeElapsed = timeit.default_timer() - start_time\n",
    "    return memory[0], timeElapsed,return_val\n",
    "\n",
    "iris = load_iris()\n",
    "dataSet=iris.data\n",
    "random_seed = 10312003\n",
    "ranges = np.random.RandomState(random_seed)\n",
    "permutation = ranges.permutation(len(dataSet))\n",
    "dataSet=dataSet[permutation]\n",
    "ourMemory,ourTime,ourCentroids= profiler(onlineKmeans, dataSet, k=3, b=33, maxiter=1000)\n",
    "minibatch=MiniBatchKMeans(n_clusters=3,max_iter=100,batch_size=33)\n",
    "memory, time, rval = profiler(minibatch.fit,dataSet)\n",
    "\n",
    "inputSize= [[2**8, 32],[2**10, 32],[2**12, 32],[2**14, 32]]\n",
    "                       \n",
    "scaler = StandardScaler()\n",
    "plt.ion()\n",
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "\n",
    "for num_samples, num_dimension in inputSize:\n",
    "    X,y = make_blobs(n_samples=num_samples, n_features=num_dimension, centers=6)\n",
    "    scaler.fit(dataSet)\n",
    "    dataSet_scaled = scaler.transform(dataSet)\n",
    "    ourMemory,ourTime,ourCentroids= profiler(onlineKmeans, dataSet, k=3, b=33, maxiter=1000)\n",
    "    memory, time, rval = profiler(minibatch.fit,dataSet)\n",
    "    plt.scatter(num_samples, memory,color=\"red\")\n",
    "    plt.show()\n",
    "    #plt.savefig('memory.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from memory_profiler import memory_usage\n",
    "import timeit\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "print \"Starting script...\"\n",
    "\n",
    "\n",
    "from timeit import Timer\n",
    "from functools import partial\n",
    "\n",
    "def getPermutation(totalRange,numberElements):\n",
    "    random_seed = 10312003\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    permutation = rng.permutation(totalRange)\n",
    "    return permutation[:numberElements]\n",
    "\n",
    "\n",
    "\n",
    "def mykmeans_plus_plus(X, max_clusters = 8, max_iterations=1000):\n",
    "    randState = 10312003\n",
    "    rnd = np.random.RandomState(randState)\n",
    "    centroids=generateStartingCentroid(X,max_clusters)\n",
    "\n",
    "    minInertia = 0\n",
    "    inertia = 0\n",
    "    for nIter in range(0, max_iterations):\n",
    "        distances = pairwise_distances(X, centroids, metric='euclidean')\n",
    "        clusters = np.argmin(distances,axis=1)\n",
    "        min_distances = np.amin(distances, axis=1)\n",
    "        sum_distortion = min_distances.sum()\n",
    "        inertia = np.sum(np.amin(distances, axis=1))  \n",
    "        if (minInertia ==0 or inertia < minInertia):\n",
    "            minInertia = inertia\n",
    "            notImproveCount=0\n",
    "        elif minInertia !=0 and inertia >= minInertia:\n",
    "            notImproveCount+=1\n",
    "        if (notImproveCount>10):\n",
    "            print \"iteration: \", nIter, \" inertia: \",inertia, \" minInertia: \",minInertia   \n",
    "            break\n",
    "        data = np.concatenate([X, clusters[:,np.newaxis]], axis=1)\n",
    "        for cRange in range(0,max_clusters):\n",
    "            allpoints = data[np.where(data[:,(data.shape[1] - 1)] == cRange)][:,range(0, data.shape[1] -1)]\n",
    "            centroids[cRange] = np.sum(allpoints, axis=0)/allpoints.shape[0]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def generateStartingCentroid(X,maxClusternumbers):\n",
    "    nPoint,dimension = X.shape\n",
    "    centroids=np.zeros([maxClusternumbers,dimension])\n",
    "    getPermutation\n",
    "    randState = 10312003\n",
    "    rnd = np.random.RandomState(randState)\n",
    "    centroids[0] = X[rnd.permutation(len(X))[0]]\n",
    "    for i in (range(1,maxClusternumbers)):\n",
    "        distances = pairwise_distances(X, centroids[:i], metric='euclidean')\n",
    "        d2weighting=np.power(np.min(distances,axis=1),2)\n",
    "        d2weighting = d2weighting/np.sum(d2weighting)\n",
    "        allIndex = range(len(X))\n",
    "        index = np.random.choice(allIndex, p=d2weighting)\n",
    "        centroids[i]=X[index]\n",
    "    return centroids\n",
    "\n",
    "def profile_memory_and_time(function, *args, **kwargs):\n",
    "    #print \"args is \",args\n",
    "    start_time = timeit.default_timer()\n",
    "    memory, return_val = memory_usage((function, (args), kwargs), max_usage=True, retval=True)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    return memory[0], elapsed,return_val\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_plus_plus=KMeans(n_clusters=10, init='k-means++', n_init=10, max_iter=100)\n",
    "\n",
    "inputSizesToGenerate = [[2**8, 32],[2**10, 32],[2**12, 32],[2**14, 32],[2**16, 32],[2**18, 32],[2**20, 32]]\n",
    "\t\t\t#,[2**22, 32],[2**24, 32],[2**26, 32],[2**28, 32],[2**30, 32],[2**32, 32]];\n",
    "\n",
    "scaler = StandardScaler()\n",
    "plt.ion()\n",
    "f1 = plt.figure()\n",
    "ax1 = f1.add_subplot(111)\n",
    "ourTimes=[]\n",
    "times=[]\n",
    "ourMemories=[]\n",
    "memoris=[]\n",
    "numSamples=[]\n",
    "\n",
    "for num_samples, num_dimension in inputSizesToGenerate:\n",
    "    print \"Running for {0} samples of dimension {1}\".format(num_samples, num_dimension)\n",
    "    X,y = make_blobs(n_samples=num_samples, n_features=num_dimension, centers=6)\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    ourMemory,ourTime,ourCentroids= profile_memory_and_time(mykmeans_plus_plus, X, max_clusters = 10, max_iterations=1000)\n",
    "    ourTimes.append(ourTime)\n",
    "    ourMemories.append(ourMemory)\n",
    "    memory, time, rval = profile_memory_and_time(kmeans_plus_plus.fit,X)\n",
    "    times.append(time)\n",
    "    memoris.append(memory)\n",
    "    numSamples.append(num_samples)\n",
    "plt.scatter(numSamples, ourMemories,color=\"green\",label=\" Our Keameans++\")\n",
    "plt.scatter(numSamples, memoris,color=\"red\",label=\"sklearn Kmeans++\")\n",
    "plt.title(\"MEMORY\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "plt.savefig('memory.png')\n",
    "plt.clf()\n",
    "plt.scatter(numSamples, ourTimes,color=\"green\",label=\"Our Keameans++\")\n",
    "plt.scatter(numSamples, times,color=\"red\",label=\"sklearn Kmeans++\")\n",
    "plt.title(\"TIME\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "plt.savefig('time.png')\n",
    "print \"Script ended. Results:\"\n",
    "print \"numSamples: \",numSamples\n",
    "print \"ourMemories: \",ourMemories\n",
    "print \"kmeansMemory: \",memoris\n",
    "print \"ourTimes: \", ourTimes\n",
    "print \"kmeansTimes: \",times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by read\n",
      "0.0\n",
      "Done with Vocab\n",
      "Now Make VLAD\n",
      "After VLAD\n",
      "Applying ANN Now\n",
      "Time taken by vlad\n",
      "Mean Avg Prec\n",
      "0.00212906287259\n",
      "16.8660001755\n",
      "Testing path\n",
      "C:/Users/ankur/Downloads/jpg\\131500.jpg\n",
      "Testing path\n",
      "C:/Users/ankur/Downloads/jpg\\131501.jpg\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import euclidean\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class ExtractSift:\n",
    "    def __init__(self, bins):\n",
    "        # store the number of bins the histogram will use\n",
    "        self.bins = bins\n",
    " \n",
    "    def describe(self, image):\n",
    "        hist = cv2.calcHist([image], [0, 1, 2],\n",
    "        None, self.bins, [0, 256, 0, 256, 0, 256])\n",
    "        hist = cv2.normalize(hist)\n",
    "        return hist.flatten()\n",
    "        \n",
    "        #previous code endsflatten\n",
    "        #descriptor_mat=[]\n",
    "        #descriptor_mat = np.zeros()\n",
    "        #sift=cv2.SIFT(4)        \n",
    "        #image=cv2.resize(image, (500, 300), interpolation=cv2.INTER_AREA)\n",
    "        #gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #gray=cv2.equalizeHist(gray)\n",
    "        #kp, des=sift.detectAndCompute(gray,None)        \n",
    "        #descriptor_mat.append(des)       \n",
    "        #descriptor_mat= np.asarray(descriptor_mat)\n",
    "        #descriptor_mat=descriptor_mat.flatten()        \n",
    "        #return descriptor_mat\n",
    "        #svd = TruncatedSVD(n_components=0, algorithm='randomized', n_iter=3, random_state=None, tol=0.0)\n",
    "        #pca = PCA(n_components=4)\n",
    "        #for pic in Training:\n",
    "        #kp, des = cv2.SIFT().detectAndCompute(image, None)\n",
    "        #descriptors = np.append(descriptors, des)\n",
    "        #svd.fit_transform(des)\n",
    "        #newmatrix = svd.transform(des)\n",
    "        #newmatrix.shape\n",
    "        #print \"New Matrix value\"\n",
    "        #print newmatrix.shape\n",
    "        #des = pca.transform(des)\n",
    "        #print \"inside des\"\n",
    "        #print des\n",
    "        #print \"New Matrix Again\"\n",
    "        #newmatrix = newmatrix.flatten()\n",
    "        #print newmatrix.shape\n",
    "        #return des.flatten()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#pca.fit(X_scaled)\n",
    "#X_pca = pca.transform(X_scaled)\n",
    "    \n",
    "import argparse\n",
    "import cPickle\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import LSHForest as LSHForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#initialize the index dictionary to store our our quantifed\n",
    "# images, with the 'key' of the dictionary being the image\n",
    "# filename and the 'value' our computed features\n",
    "index = {}\n",
    "\n",
    "def createVocabulary(data,k,itr):\n",
    "    clust_centers, labels = mykmeansplusplus(data,k,itr)\n",
    "    f = open(\"C:/Index-Project/ankur.txt\", \"w\")\n",
    "    f.write(cPickle.dumps(index))\n",
    "    f.close()\n",
    "    return clust_centers\n",
    "\n",
    "def mykmeansplusplus (points,k,itr):\n",
    "    data_x_shape = points.shape[0]\n",
    "    index = np.random.randint(data_x_shape, size =1)\n",
    "    initial_clust_center = points[index]\n",
    "    initialCentroid = points[index]\n",
    "    weight = np.zeros((points.shape[0]))\n",
    "    index = np.zeros((points.shape[0]))\n",
    "\n",
    "    for i in range(0,(k - 1)):\n",
    "        Y= cdist(points, initialCentroid, metric='euclidean', p=2, V=None, VI=None, w=None)\n",
    "        min_dist = np.amin(Y, axis=1) \n",
    "        min_center = np.argmin(Y, axis=1)\n",
    "        min_dist_sum = np.sum(min_dist, axis=0)\n",
    "        min_squared_dist_sum = np.sum(min_dist**2, axis=0)\n",
    "        min_dist = min_dist**2\n",
    "        min_dist_prob = min_dist / min_squared_dist_sum\n",
    "        next_index = np.random.choice(points.shape[0],1,p=min_dist_prob)\n",
    "        initial_clust_center = np.concatenate((initial_clust_center,points[next_index]),axis=0)\n",
    "        \n",
    "    initial_class = np.zeros((points.shape[0]))\n",
    "    for j in range(0,itr):\n",
    "        Z=cdist(points, initial_clust_center, metric='euclidean', p=2, V=None, VI=None, w=None)\n",
    "        target_dist = np.amin(Z, axis=1)  \n",
    "        target_class = np.argmin(Z, axis=1)\n",
    "        for i in range(0,k-1):\n",
    "            values = points[target_class == i]\n",
    "            #print(values)\n",
    "            #print(i)\n",
    "            initialCentroid[i,:] = np.mean(values, axis = 0)\n",
    "        if np.array_equal(initial_class,target_class):\n",
    "            break\n",
    "        else:\n",
    "            initial_class = target_class\n",
    "\n",
    "    return initial_clust_center, target_class  \n",
    "\n",
    "# initialize our image descriptor -- a 3D RGB histogram with\n",
    "# 8 bins per channel\n",
    "desc = ExtractSift([8, 8, 8])\n",
    "feature_size = math.pow(8, 3)\n",
    "def read():\n",
    "    path = \"C:/Users/ankur/Downloads/jpg/\"\n",
    "    print(\"Path inside read\")\n",
    "    print path\n",
    "    imlist = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    \n",
    "    \n",
    "    featuresCollect = np.zeros([len(imlist), feature_size])\n",
    "    i=0\n",
    "    # use glob to grab the image paths and loop over them\n",
    "    for imagePath in glob.glob(\"C:/Users/ankur/Downloads/jpg\" + \"/*.jpg\"):\n",
    "        # extract our unique image ID (i.e. the filename)\n",
    "        k = imagePath[imagePath.rfind(\"/\") + 1:]\n",
    "        # load the image, describe it using our RGB histogram\n",
    "        # descriptor, and update the index\n",
    "        image = cv2.imread(imagePath)\n",
    "        features = desc.describe(image)\n",
    "        preprocessing.scale(features,axis = 0, with_mean=True, with_std=False)\n",
    "        featuresCollect[i] = features \n",
    "        #print(\"Added a feature desc\")\n",
    "        i=i+1\n",
    "        index[k] = features\n",
    "    clusters = createVocabulary(featuresCollect,2,100)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "print \"Time taken by read\"\n",
    "t0 = time.time()\n",
    "#read()\n",
    "print (time.time()-t0)\n",
    "    \n",
    "print \"Done with Vocab\"\n",
    "#print vocab    \n",
    "\n",
    "def myvlad(local_descriptors, centroids):\n",
    "    V = np.zeros([centroids.shape[0],local_descriptors.shape[1]])\n",
    "    distances = pairwise_distances(local_descriptors, centroids, metric='euclidean')\n",
    "    clusters = np.argmin(distances,axis=1)\n",
    "    for iter, center in enumerate(centroids):\n",
    "        points_belonging_to_cluster = local_descriptors[clusters == iter]\n",
    "        V[iter] = np.sum(points_belonging_to_cluster - center, axis=0)\n",
    "        \n",
    "    return V\n",
    "\n",
    "print \"Now Make VLAD\"\n",
    "#vlad = my_vlad(featuresCollect,vocab)\n",
    "#print vlad\n",
    "print \"After VLAD\"\n",
    "\n",
    "print \"Applying ANN Now\"\n",
    "\n",
    "#approx_neighbors = lshf.kneighbors(vlad,4,return_distance=True)\n",
    "#print approx_neighbors\n",
    "    \n",
    "# we are now done indexing our image -- now we can write our\n",
    "# index to disk\n",
    "\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    " \n",
    "class VLAD:\n",
    "    def __init__(self, index):\n",
    "        # store our index of images\n",
    "        self.index = index\n",
    "      \n",
    " \n",
    "    def my_vlad(self, queryFeatures):\n",
    "        # initialize our dictionary of results\n",
    "        results = {}\n",
    "       \n",
    "        mAP=0.0\n",
    "        numberOfMatch=0\n",
    "        c=0\n",
    " \n",
    "        # loop over the index\n",
    "        for (k, features) in self.index.items():\n",
    "            c=c+1\n",
    "            d = self.chi2_distance(features, queryFeatures)\n",
    "            results[k] = d\n",
    "            if (d<=1.5):\n",
    "                mAP=mAP+(1.0/c)\n",
    "                numberOfMatch=numberOfMatch+1\n",
    "                \n",
    "        mAP=mAP/numberOfMatch\n",
    "           \n",
    "        print \"Mean Avg Prec\"\n",
    "        print mAP\n",
    " \n",
    "        # sort our results, so that the smaller distances (i.e. the\n",
    "        # more relevant images are at the front of the list)\n",
    "        results = sorted([(v, k) for (k, v) in results.items()])\n",
    "        #print (\"Array of Distances\")\n",
    "        #print results\n",
    "        \n",
    "        \n",
    "        ####Precision - Recall Curve\n",
    "        rankedLists = results\n",
    "        numViewedImages = 5\n",
    "        for ri in xrange(0, numViewedImages):\n",
    "            # The first image is the query image itself\n",
    "            (score, imgId) = results[ri+1]\n",
    "            #print imageName\n",
    "            imgPath = \"C:/Users/ankur/Downloads/%s\" % (imgId)\n",
    "            #imgId = rankedLists[ri+1]\n",
    "            \n",
    "            #imgPath = \"C:/Users/ankur/Downloads/%s\"+imgId[1]\n",
    "            plt.subplot(5,numViewedImages/5,ri) \n",
    "            #plt.subimage(imread(imgPath))\n",
    "        plt.show()\n",
    "\n",
    "        # return our results\n",
    "        return results\n",
    "    \n",
    " \n",
    "    def chi2_distance(self, histA, histB, eps = 1e-10):\n",
    "        # compute the chi-squared distance\n",
    "        d = 0.5 * np.sum([((a - b) ** 2) / (a + b + eps)\n",
    "            for (a, b) in zip(histA, histB)])\n",
    "        # return the chi-squared distance\n",
    "        return d\n",
    "    \n",
    "\n",
    "# import the necessary packages\n",
    "#from pyimagesearch.searcher import Searcher\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cPickle\n",
    "import cv2\n",
    "\n",
    "# load the index and initialize our searcher\n",
    "index = cPickle.loads(open(\"C:/Index-Project/ankur.txt\").read())\n",
    "searcher = VLAD(index)\n",
    "\n",
    "\n",
    "# loop over images in the index -- we will use each one as\n",
    "# a query image\n",
    "cv2.startWindowThread()\n",
    "#for (query, queryFeatures) in index.items():\n",
    "# perform the search using the current query\n",
    "print \"Time taken by vlad\"\n",
    "t0 = time.time()\n",
    "\n",
    "query=\"131500.jpg\"\n",
    "path =\"C:/\" + \"/Anu/%s\" % (query)\n",
    "#print path\n",
    "image = cv2.imread(path)\n",
    "#print image\n",
    "queryFeatures = desc.describe(image)\n",
    "\n",
    "results = searcher.my_vlad(queryFeatures)\n",
    "\n",
    "print (time.time()-t0)\n",
    " \n",
    "# load the query image and display itqueryFeatures\n",
    "#print query\n",
    "#print queryFeatures\n",
    "\n",
    "#print path\n",
    "queryImage = cv2.imread(path)\n",
    "#print queryImage\n",
    "#print queryImage\n",
    "cv2.imshow('Query', queryImage)\n",
    "cv2.waitKey(0) & 0xFF\n",
    "#print \"query: %s\" % (query)\n",
    " \n",
    "# initialize the two montages to display our results --\n",
    "# we have a total of 25 images in the index, but let's only\n",
    "# display the top 10 results; 5 images per montage, with\n",
    "# images that are 400x166 pixels\n",
    "#montageA = np.zeros((3264*5,2448,3), dtype = \"uint8\")\n",
    "#montageB = np.zeros((3264*5,2448,3), dtype = \"uint8\")\n",
    "\n",
    "# loop over the top ten results\n",
    "\n",
    "\n",
    "#Code to output Mean Average Precision\n",
    "\n",
    "\n",
    "\n",
    "for j in xrange(0, 2):\n",
    "    # grab the result (we are using row-major order) and\n",
    "    # load the result image\n",
    "    (score, imageName) = results[j]\n",
    "    #print imageName\n",
    "    path = \"C:/Users/ankur/Downloads/%s\" % (imageName)\n",
    "    print \"Testing path\"\n",
    "    print path\n",
    "    result = cv2.imread(path)\n",
    "    cv2.imshow(\"Results 1-5\", result)\n",
    "    cv2.waitKey(0) & 0xFF\n",
    "\n",
    "cv2.waitKey(0) & 0xFF\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "class RGBHistogram:\n",
    "\tdef __init__(self, bins):\n",
    "\t\t# store the number of bins the histogram will use\n",
    "\t\tself.bins = bins\n",
    " \n",
    "\tdef describe(self, image):\n",
    "\t\t# compute a 3D histogram in the RGB colorspace,\n",
    "\t\t# then normalize the histogram so that images\n",
    "\t\t# with the same content, but either scaled larger\n",
    "\t\t# or smaller will have (roughly) the same histogram\n",
    "\t\thist = cv2.calcHist([image], [0, 1, 2],\n",
    "\t\t\tNone, self.bins, [0, 256, 0, 256, 0, 256])\n",
    "\t\thist = cv2.normalize(hist)\n",
    " \n",
    "\t\t# return out 3D histogram as a flattened array\n",
    "\t\treturn hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyimagesearch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8cff7a4d39bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import the necessary packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyimagesearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyimagesearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRGBHistogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyimagesearch"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import pyimagesearch\n",
    "from pyimagesearch import RGBHistogram\n",
    "import argparse\n",
    "import cPickle\n",
    "import glob\n",
    "import cv2\n",
    " \n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", required = True,\n",
    "\thelp = \"Path to the directory that contains the images to be indexed\")\n",
    "ap.add_argument(\"-i\", \"--index\", required = True,\n",
    "\thelp = \"Path to where the computed index will be stored\")\n",
    "args = vars(ap.parse_args())\n",
    " \n",
    "# initialize the index dictionary to store our our quantifed\n",
    "# images, with the 'key' of the dictionary being the image\n",
    "# filename and the 'value' our computed features\n",
    "index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ca51a5191ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'single' is not defined"
     ]
    }
   ],
   "source": [
    "d= 128\n",
    "n=512\n",
    "nq=2000\n",
    "x = single(randn(d,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/subj1_train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-67819f2c22cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Load and scale data. just for one file. Write the for loop for all file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/processed/subj1_train_data.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/subj1_train_data.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Load and scale data. just for one file. Write the for loop for all file \n",
    "X = np.load(\"../data/processed/subj1_train_data.npy\")\n",
    "events,dim,vectors = X.shape\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "#MiniBatch Kmeans:\n",
    "minibatch = MiniBatchKMeans(n_clusters=10,max_iter=100,batch_size=33)\n",
    "\n",
    "#Pipeline:\n",
    "galPPLine = Pipeline(steps=[\n",
    "        ('pca', pca),\n",
    "        ('minibatch', minibatch)\n",
    "        ])\n",
    "\n",
    "for i in range(events):\n",
    "        scaler.fit(X[i])\n",
    "        X_scaled = scaler.transform(X[i])\n",
    "        fittedData = galPPLine.set_params(pca__n_components=10,\n",
    "                       minibatch__n_clusters=3,\n",
    "                       minibatch__batch_size=10).fit(X_scaled)\n",
    "        break\n",
    "\n",
    "testVector = X[0].T[0]\n",
    "print testVector.reshape(-1, 1).shape\n",
    "print fittedData.predict(testVector.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
